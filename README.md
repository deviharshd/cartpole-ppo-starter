# CartPole PPO Starter

This project is a clean, self‑contained example of training and evaluating a Proximal Policy Optimization (PPO) agent on the classic CartPole‑v1 environment using Stable‑Baselines3 and Gymnasium.

## Contents

- `cartpole/`
  - `cartpole_ppo_train.py` – train PPO on CartPole‑v1.
  - `cartpole_ppo_infer.py` – watch the trained policy.
  - `cartpole_ppo.ipynb` – notebook walkthrough.
  - `cartpole_eval.py` – evaluation and summary stats.
  - `cartpole_record_video.py` – record CartPole runs to video.
- `notes/`
  - `rl_basics.md` – reinforcement learning fundamentals.
  - `ppo_explained.md` – PPO overview.
  - `experiment_template.md` – template for logging experiments.
- `images/`
  - `cartpole_graph.png` – training reward curve (generated by training).

## Setup

```bash
python -m venv .venv
source .venv/bin/activate      # Linux/macOS
# or
.venv\Scripts\activate         # Windows

python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

## Usage

Train PPO:

```bash
python cartpole/cartpole_ppo_train.py --timesteps 200000
```

Evaluate:

```bash
python cartpole/cartpole_eval.py --episodes 20
```

Record videos (requires `moviepy`):

```bash
python cartpole/cartpole_record_video.py --episodes 3
```

Visualize the trained agent:

```bash
python cartpole/cartpole_ppo_infer.py --episodes 5
```

## Demo Video

<video src="https://github.com/deviharshd/cartpole-ppo-starter/raw/main/videos/cartpole/cartpole_ppo-episode-0.mp4" controls width="480">
  Your browser does not support the video tag.
</video>
