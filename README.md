# CartPole PPO Starter

This repository trains and evaluates a Proximal Policy Optimization (PPO)
agent on the classic **CartPole‑v1** environment using
**Stable‑Baselines3** and **Gymnasium**. It also produces a simple
learning‑curve plot and optional rollout videos.

CartPole frame:

![CartPole environment](images/cartpole_env.png)

---

## Project layout

- `cartpole/`
  - `cartpole_ppo_train.py` – train PPO on CartPole‑v1 (CLI options for timesteps, seed, etc.).
  - `cartpole_ppo_infer.py` – run the trained policy and render episodes.
  - `cartpole_ppo.ipynb` – notebook version of the training/eval workflow.
  - `cartpole_eval.py` – evaluate the trained model and print summary statistics.
  - `cartpole_record_video.py` – record episodes to `.mp4` files.
- `notes/`
  - `rl_basics.md` – reinforcement learning fundamentals.
  - `ppo_explained.md` – short PPO overview.
  - `experiment_template.md` – small template for logging runs.
- `images/`
  - `cartpole_env.png` – single frame of the environment.
  - `cartpole_graph.png` – reward curve generated by the training script.

---

## Setup

From this project's root (`cartpole-ppo-starter/`):

```bash
python -m venv .venv
source .venv/bin/activate      # Linux/macOS
# or
.venv\Scripts\activate         # Windows

python -m pip install --upgrade pip
python -m pip install -r requirements.txt
```

---

## Training

Basic run:

```bash
python cartpole/cartpole_ppo_train.py --timesteps 200000
```

Useful options:

- `--timesteps` – total environment steps (e.g. `200000`).
- `--seed` – random seed (default `0`).
- `--model-name` – base name for the saved model (default `cartpole_ppo_model`).
- `--learning-rate` – PPO learning rate (default `3e-4`).
- `--batch-size` – PPO batch size (default `64`).

Training writes:

- Model checkpoint:
  - `cartpole/models/cartpole_ppo_model.zip`
- Reward curve images:
  - `cartpole/training_plot.png`
  - `images/cartpole_graph.png`

Example training curve:

![CartPole PPO training curve](images/cartpole_graph.png)

---

## Evaluation

After training, evaluate the model over several episodes:

```bash
python cartpole/cartpole_eval.py --episodes 20
```

This prints:

- Mean return.
- Standard deviation of returns.
- Minimum and maximum return.

---

## Running the agent

To watch the trained agent in a render window:

```bash
python cartpole/cartpole_ppo_infer.py --episodes 5
```

This uses `render_mode="human"` so you can see the cart and pole in real time.

---

## Recording videos

To record `.mp4` videos of the trained agent:

```bash
python cartpole/cartpole_record_video.py --episodes 3
```

Videos are written to:

```text
videos/cartpole/
```

For example:

- `videos/cartpole/cartpole_ppo-episode-0.mp4`
- `videos/cartpole/cartpole_ppo-episode-1.mp4`

On GitHub, click the link below to open one of the videos:

- [CartPole PPO video](https://github.com/deviharshd/cartpole-ppo-starter/blob/main/videos/cartpole/cartpole_ppo-episode-0.mp4)

Or click the training curve image:

[![CartPole PPO video](images/cartpole_graph.png)](https://github.com/deviharshd/cartpole-ppo-starter/blob/main/videos/cartpole/cartpole_ppo-episode-0.mp4)

---

## Notes

- `notes/rl_basics.md` – quick reminder of core RL concepts.
- `notes/ppo_explained.md` – short write‑up of PPO.
- `notes/experiment_template.md` – simple skeleton for tracking runs.

